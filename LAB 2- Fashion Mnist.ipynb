{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"LAB 2- Fashion Mnist.ipynb","provenance":[{"file_id":"12KKmBPIdCuSJ9rgQ13auOJm7BrgMJVb0","timestamp":1601918962235},{"file_id":"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l03c01_classifying_images_of_clothing.ipynb","timestamp":1596043085373}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jYysdyb-CaWM"},"source":["# Classifying Images of Clothing"]},{"cell_type":"markdown","metadata":{"id":"FbVhjPpzn6BM"},"source":["In this tutorial, we'll build and train a neural network to classify images of clothing, like sneakers and shirts.\n","\n","Classification is widely used in Artificial intelligence in several fields. Also, there are many ways to build classifiers using machine learning and deep learning methods.\n","\n","This guide uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow."]},{"cell_type":"markdown","metadata":{"id":"H0tMfX2vR0uD"},"source":["## Install and import dependencies\n","\n","We will use a dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/). This API simplifies downloading and accessing datasets, and provides several sample datasets to work with. We're also using a few helper libraries."]},{"cell_type":"code","metadata":{"id":"P7mUJVqcINSM"},"source":["!pip install -U tensorflow_datasets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kOA5qUl9_-Go"},"source":["We proceed to import dependencies."]},{"cell_type":"code","metadata":{"id":"1UbK0Uq7GWaO"},"source":["# Import Tensorflow\n","import tensorflow as tf\n","# Import TensorFlow Datasets\n","import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()\n","\n","# Helper libraries\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nfNnMTg7AOpL"},"source":["Also, we will set the logger for tensor flow to only show us error messages."]},{"cell_type":"code","metadata":{"id":"590z76KRGtKk"},"source":["import logging\n","logger = tf.get_logger()\n","logger.setLevel(logging.ERROR) #You can comment this line to see all the outputs."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yR0EdgrLCaWR"},"source":["## Import the MNIST dataset"]},{"cell_type":"markdown","metadata":{"id":"DLdCchMdCaWQ"},"source":["This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 $\\times$ 28 pixels), as seen here:\n","\n","<table>\n","  <tr><td>\n","    <img src=\"https://storage.googleapis.com/tfds-data/visualization/fig/mnist-3.0.1.png\"\n","         alt=\"MNIST\" width=\"600\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n","  </td></tr>\n","</table>\n","\n","We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow, using the [Datasets](https://www.tensorflow.org/datasets) API:"]},{"cell_type":"markdown","metadata":{"id":"7UOvA-oRBPC-"},"source":["The following line imports and unpackages the dataset. In this case the function receives the following parameters:\n","\n","\n","*   `name=\"mnist\"`: The name of the dataset that we want to import. This name has to be exact in order to find the dataset. You can see other datasets names in the [Datasets](https://www.tensorflow.org/datasets) API:\n","*   `as_supervised=True`: The returned dataset will have a 2-tuple structure (input, label). If as_supervised were False, the returned dataset will have a dictionary with all the features\n","*   `with_info=True`: If with_info is True, the function will return a tuple where the first element would be the dataset information and the second would be metadata relevant to the dataset (version, features, num-examples,...)\n","\n","More arguments for this function can be found in the [Load function documentation](https://www.tensorflow.org/datasets/api_docs/python/tfds/load)\n","\n"]},{"cell_type":"code","metadata":{"id":"7MqDQO0KCaWS"},"source":["dataset, metadata = tfds.load(name='mnist', as_supervised=True, with_info=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wtftlQxXDkaB"},"source":["We divide the dataset into its training and test sub datasets. We said that we will divide our dataset into 60000 images for training and 10000 images for testing. We don't need to do this division because the dataset is already divided, there will be some cases in which we will need to split the dataset, so there is an argument for the load function that makes that for us, check the split argument in the [Load funtion documentation](https://www.tensorflow.org/datasets/api_docs/python/tfds/load)"]},{"cell_type":"code","metadata":{"id":"IHZzHM9PBL4K"},"source":["train_dataset, test_dataset = dataset['train'], dataset['test']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9FDsUlxCaWW"},"source":["Loading the dataset returns metadata as well as a *training dataset* and *test dataset*.\n","\n","* The model is trained using `train_dataset`.\n","* The model is tested against `test_dataset`.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Brm0b_KACaWX"},"source":["### Explore the data\n","\n","Let's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, and 10000 images in the test set:"]},{"cell_type":"code","metadata":{"id":"MaOTZxFzi48X"},"source":["num_train_examples = metadata.splits['train'].num_examples\n","num_test_examples = metadata.splits['test'].num_examples\n","print(\"Number of training examples: {}\".format(num_train_examples))\n","print(\"Number of test examples:     {}\".format(num_test_examples))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ES6uQoLKCaWr"},"source":["## Preprocess the data\n","\n","The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets."]},{"cell_type":"code","metadata":{"id":"nAsH3Zm-76pB"},"source":["\n","def normalize(images, labels):\n","  images = tf.cast(images, tf.float32)\n","  images /= 255\n","  return images, labels\n","\n","# The map function applies the normalize function to each element in the train\n","# and test datasets\n","train_dataset =  train_dataset.map(normalize)\n","test_dataset  =  test_dataset.map(normalize)\n","\n","# The first time you use the dataset, the images will be loaded from disk\n","# Caching will keep them in memory, making training faster\n","train_dataset =  train_dataset.cache()\n","test_dataset  =  test_dataset.cache()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lIQbEiJGXM-q"},"source":["### Explore the processed data\n","\n","Let's plot an image to see what it looks like."]},{"cell_type":"code","metadata":{"id":"oSzE9l7PjHx0"},"source":["# Take a single image, and remove the color dimension by reshaping\n","image, label = tf.data.experimental.get_single_element(test_dataset.take(1))\n","\n","image = image.numpy().reshape((28,28))\n","# Plot the image\n","plt.figure()\n","plt.imshow(image, cmap=plt.cm.binary)\n","plt.colorbar()\n","plt.grid(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ee638AlnCaWz"},"source":["Display the first 10 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network."]},{"cell_type":"code","metadata":{"id":"oZTImqg_CaW1"},"source":["plt.figure(figsize=(10,10))\n","i = 0\n","for (image, label) in test_dataset.take(10):\n","    image = image.numpy().reshape((28,28))\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(image, cmap=plt.cm.binary)\n","    plt.xlabel(label.numpy())\n","    i += 1\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59veuiEZCaW4"},"source":["## Build the model\n","\n","Building the neural network requires configuring the layers of the model, then compiling the model."]},{"cell_type":"markdown","metadata":{"id":"Gxg1XGm0eOBy"},"source":["### Setup the layers\n","\n","The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n","\n","Much of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training."]},{"cell_type":"code","metadata":{"id":"9ODch-OFCaW4"},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n","    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gut8A_7rCaW6"},"source":["This network has three layers:\n","\n","* **input** `tf.keras.layers.Flatten` — This layer transforms the images from a 2d-array of 28 $\\times$ 28 pixels, to a 1d-array of 784 pixels (28\\*28). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn, as it only reformats the data.\n","\n","* **\"hidden\"** `tf.keras.layers.Dense`— A densely connected layer of 128 neurons. Each neuron (or node) takes input from all 784 nodes in the previous layer, weighting that input according to hidden parameters which will be learned during training, and outputs a single value to the next layer.\n","\n","* **output**  `tf.keras.layers.Dense` — A 128-neuron, followed by 10-node *softmax* layer. Each node represents a class of clothing. As in the previous layer, the final layer takes input from the 128 nodes in the layer before it, and outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1.\n","\n","\n","\n","### Compile the model\n","\n","Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n","\n","\n","* *Loss function* — An algorithm for measuring how far the model's outputs are from the desired output. The goal of training is this measures loss. For more information about this and other loss functions, see the [Losses documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n","* *Optimizer* —An algorithm for adjusting the inner parameters of the model in order to minimize loss. For more information about the available optimizers see the [optimizers documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n","* *Metrics* —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified. There are others metrics for the models, see the [metrics documentation](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) for more information"]},{"cell_type":"code","metadata":{"id":"Lhan11blCaW7"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy','mse','mae'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKF6uW-BCaW-"},"source":["## Train the model\n","\n","First, we define the iteration behavior for the train dataset:\n","1. Repeat forever by specifying `dataset.repeat()` (the `epochs` parameter described below limits how long we perform training).\n","2. The `dataset.shuffle(60000)` randomizes the order so our model cannot learn anything from the order of the examples.\n","3. And `dataset.batch(32)` tells `model.fit` to use batches of 32 images and labels when updating the model variables. This number can be changed depending of the capabilities of our hardware.\n","\n","Training is performed by calling the `model.fit` method:\n","1. Feed the training data to the model using `train_dataset`.\n","2. The model learns to associate images and labels by adjusting the internal parameter of each one of the neurons and its biases.\n","3. The `epochs=5` parameter limits training to 5 full iterations of the training dataset, so a total of 5 * 60000 = 300000 examples.\n","\n"]},{"cell_type":"code","metadata":{"id":"o_Dp8971McQ1"},"source":["BATCH_SIZE = 32\n","train_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n","test_dataset = test_dataset.cache().batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvwvpA64CaW_"},"source":["model.fit(train_dataset, epochs=5, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3ZVOhugCaXA"},"source":["As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.89 (or 89%) on the training data."]},{"cell_type":"markdown","metadata":{"id":"oEw4bZgGCaXB"},"source":["## Evaluate accuracy\n","\n","Next, compare how the model performs on the test dataset. Use all examples we have in the test dataset to assess accuracy."]},{"cell_type":"code","metadata":{"id":"VflXLEeECaXC"},"source":["test_loss, test_accuracy, test_mse, test_mae = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n","print('Accuracy on test dataset:', test_accuracy)\n","print('MSE on test dataset:', test_mse)\n","print('MAE on test dataset:', test_mae)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yWfgsmVXCaXG"},"source":["As it turns out, the accuracy on the test dataset is smaller than the accuracy on the training dataset. This is completely normal, since the model was trained on the `train_dataset`. When the model sees images it has never seen during training, (that is, from the `test_dataset`), we can expect performance to go down. "]},{"cell_type":"markdown","metadata":{"id":"xsoS7CPDCaXH"},"source":["## Make predictions and explore\n","\n","With the model trained, we can use it to make predictions about some images."]},{"cell_type":"code","metadata":{"id":"Ccoz4conNCpl"},"source":["for test_images, test_labels in test_dataset.take(1):\n","  test_images = test_images.numpy()\n","  test_labels = test_labels.numpy()\n","  predictions = model.predict(test_images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gl91RPhdCaXI"},"source":["predictions.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9Kk1voUCaXJ"},"source":["Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:"]},{"cell_type":"code","metadata":{"id":"3DmJEUinCaXK"},"source":["predictions[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hw1hgeSCaXN"},"source":["A prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:"]},{"cell_type":"code","metadata":{"id":"qsqenuPnCaXO"},"source":["np.argmax(predictions[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E51yS7iCCaXO"},"source":["So the model is most confident that this image is a shirt, or `class_names[6]`. And we can check the test label to see this is correct:"]},{"cell_type":"code","metadata":{"id":"Sd7Pgsu6CaXP"},"source":["test_labels[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygh2yYC972ne"},"source":["We can graph this to look at the full set of 10 class predictions"]},{"cell_type":"code","metadata":{"id":"DvYmmrpIy6Y1"},"source":["def plot_image(i, predictions_array, true_labels, images):\n","  predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]\n","  plt.grid(False)\n","  plt.xticks([])\n","  plt.yticks([])\n","  \n","  plt.imshow(img[...,0], cmap=plt.cm.binary)\n","\n","  predicted_label = np.argmax(predictions_array)\n","  if predicted_label == true_label:\n","    color = 'blue'\n","  else:\n","    color = 'red'\n","  \n","  plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,\n","                                100*np.max(predictions_array),\n","                                true_label),\n","                                color=color)\n","\n","def plot_value_array(i, predictions_array, true_label):\n","  predictions_array, true_label = predictions_array[i], true_label[i]\n","  plt.grid(False)\n","  plt.xticks([])\n","  plt.yticks([])\n","  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n","  plt.ylim([0, 1]) \n","  predicted_label = np.argmax(predictions_array)\n","  \n","  thisplot[predicted_label].set_color('red')\n","  thisplot[true_label].set_color('blue')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4Ov9OFDMmOD"},"source":["Let's look at the 0th image, predictions, and prediction array. "]},{"cell_type":"code","metadata":{"id":"HV5jw-5HwSmO"},"source":["i = 0\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(i, predictions, test_labels, test_images)\n","plt.subplot(1,2,2)\n","plot_value_array(i, predictions, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ko-uzOufSCSe"},"source":["i = 12\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(i, predictions, test_labels, test_images)\n","plt.subplot(1,2,2)\n","plot_value_array(i, predictions, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kgdvGD52CaXR"},"source":["Let's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident. "]},{"cell_type":"code","metadata":{"id":"hQlnbqaw2Qu_"},"source":["# Plot the first X test images, their predicted label, and the true label\n","# Color correct predictions in blue, incorrect predictions in red\n","num_rows = 6\n","num_cols = 4\n","num_images = num_rows*num_cols\n","plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n","for i in range(num_images):\n","  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n","  plot_image(i, predictions, test_labels, test_images)\n","  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n","  plot_value_array(i, predictions, test_labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R32zteKHCaXT"},"source":["Finally, use the trained model to make a prediction about a single image. "]},{"cell_type":"code","metadata":{"id":"yRJ7JU7JCaXT"},"source":["# Grab an image from the test dataset\n","img = test_images[0]\n","\n","print(img.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vz3bVp21CaXV"},"source":["`tf.keras` models are optimized to make predictions on a *batch*, or collection, of examples at once. So even though we're using a single image, we need to add it to a list:"]},{"cell_type":"code","metadata":{"id":"lDFh5yF_CaXW"},"source":["# Add the image to a batch where it's the only member.\n","img = np.array([img])\n","\n","print(img.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQ5wLTkcCaXY"},"source":["Now predict the image:"]},{"cell_type":"code","metadata":{"id":"o_rzNSdrCaXY"},"source":["predictions_single = model.predict(img)\n","\n","print(predictions_single)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Ai-cpLjO-3A"},"source":["plot_value_array(0, predictions_single, test_labels)\n","_ = plt.xticks(range(10), rotation=45)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cU1Y2OAMCaXb"},"source":["`model.predict` returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:"]},{"cell_type":"code","metadata":{"id":"2tRmdq_8CaXb"},"source":["np.argmax(predictions_single[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFc2HbEVCaXd"},"source":["And, as before, the model predicts a label of 6 (shirt)."]},{"cell_type":"markdown","metadata":{"id":"u1DJq1N3KqHs"},"source":["# Assignment: Classification on classic mnist dataset\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qpt7XSWTLaus"},"source":["Mnist datasets are called the \"hello world\" examples for deep learning engineering. In this assignment, you will have to create a deep learning model that can classify handwritten digits from the tipical mnist dataset.\n","\n","You have to\n","\n","\n","\n","1.   Import the [Mnist dataset](https://www.tensorflow.org/datasets/catalog/mnist?hl=es-419). \n","2.   Analyse and explain the dataset.\n","3.   Explore and pre-process the data. Compare the effects of normalizing and not normalizing the data in the training process.\n","4.   Propose a solution. How will you solve this classification problem?\n","5.   Create a model. Try changing the amount of neurons on each layer and explain the differences of changing these numbers. Go from very low(e.g 10) in ranges up to 512 and see how metrics change. Try changing the activation functions, how does the training process and results change?\n","6.   Train the model. Use at least 2 more metrics that helps us to understand the training process. Explain how the used metrics helps us to evaluate the trained model.\n","7.   See results. Plot a loss vs epoch chart. What would happen if you increase the epochs in the training process? How many epochs are optimum? Also, show some examples of predicted images (At least 10) with their predicted confidence.\n","\n","\n","Delivery\n","\n","You will have to make a report with the previously described tasks. Especially\n","\n","1. Introduction\n","2. Analysis of the data\n","3. Data pre-processing metodology and explanation\n","4. Proposed model (Include charts, images, etc.)\n","5. Training process explanation (optimizers used, loss functions used, data splitting metodology)\n","6. Results"]}]}